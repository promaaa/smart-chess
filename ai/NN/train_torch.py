import sys
"""
Script d'entra√Ænement PyTorch optimis√© pour GPU
Compatible avec Google Colab et machines locales avec GPU
"""
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import os

from Chess import Chess
from ai.NN.torch_nn_evaluator import TorchNNEvaluator, save_weights_npz, load_from_npz, torch_save_checkpoint, torch_load_checkpoint

# --- CONFIGURATION DE L'ENTRA√éNEMENT ---
DATASET_PATH = "C:\\Users\\gauti\\OneDrive\\Documents\\UE commande\\chessData.csv"  # Adapt√© pour Colab (fichier √† la racine)
WEIGHTS_FILE = "chess_nn_weights.npz"
CHECKPOINT_FILE = "chess_model_checkpoint.pt"

# Architecture
HIDDEN_SIZE = 256
DROPOUT = 0.3
LEAKY_ALPHA = 0.01

# Hyperparam√®tres
LEARNING_RATE = 0.0001
WEIGHT_DECAY = 1e-4  # L2 regularization (AdamW)
EPOCHS = 20
BATCH_SIZE = 128  # Plus grand pour GPU
MAX_SAMPLES = 500_000  # Plus de donn√©es avec GPU !
EVAL_MAX_SAMPLES = 5000

# Options
USE_SAMPLING = True
RESET_WEIGHTS = False
DEBUG_STATS = True

# LR Scheduler
USE_LR_SCHEDULER = True
LR_PATIENCE = 4
LR_FACTOR = 0.5

# LR Warmup
USE_LR_WARMUP = True
WARMUP_EPOCHS = 3
WARMUP_START_LR = 0.0001

# Device (auto-d√©tection GPU)
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üñ•Ô∏è  Device: {DEVICE}")
if torch.cuda.is_available():
    print(f"üöÄ GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")


class ChessDataset(Dataset):
    """Dataset PyTorch pour les positions d'√©checs"""
    def __init__(self, fens, evaluations):
        self.fens = fens
        self.evaluations = evaluations
        self.chess = Chess()
        
        # Pr√©calculer l'encodage pour acc√©l√©rer (optionnel, consomme plus de RAM)
        # self.encoded = [self._encode_fen(fen) for fen in tqdm(fens, desc="Encoding positions")]
        
    def __len__(self):
        return len(self.fens)
    
    def __getitem__(self, idx):
        fen = self.fens[idx]
        target = self.evaluations[idx]
        
        # Encoder la position
        self.chess.load_fen(fen)
        encoded = self._encode_board(self.chess)
        
        return torch.from_numpy(encoded).float(), torch.tensor([target], dtype=torch.float32)
    
    def _encode_board(self, chess_instance):
        """Encode le plateau en vecteur 768D (identique √† nn_evaluator.py)"""
        piece_to_index = {
            'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,
            'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11
        }
        vec = np.zeros(768, dtype=np.float32)
        for piece_char, bitboard in chess_instance.bitboards.items():
            if bitboard == 0:
                continue
            piece_index = piece_to_index[piece_char]
            temp_bb = int(bitboard)
            while temp_bb:
                square = (temp_bb & -temp_bb).bit_length() - 1
                vector_position = piece_index * 64 + square
                vec[vector_position] = 1.0
                temp_bb &= temp_bb - 1
        return vec


def load_data(filepath: str):
    """Charge le dataset FEN,Evaluation et le nettoie."""
    print(f"üìÇ Chargement du dataset depuis {filepath}...")
    
    df = pd.read_csv(
        filepath, 
        names=['FEN', 'Evaluation'], 
        skiprows=1,
        comment='#'
    )
    
    initial_count = len(df)
    df.dropna(inplace=True)
    cleaned_count = len(df)
    
    if initial_count > cleaned_count:
        print(f"üßπ Nettoyage : {initial_count - cleaned_count} lignes corrompues supprim√©es.")
    
    fens = df['FEN'].values
    EVAL_SCALE_FACTOR = 1000.0
    evaluations = (df['Evaluation'].astype(int).values) / EVAL_SCALE_FACTOR
    
    print(f"‚úÖ {len(fens):,} positions valides charg√©es.")
    return fens, evaluations


def evaluate_model(model, dataloader, device):
    """√âvalue le mod√®le sur un dataset"""
    model.eval()
    predictions = []
    targets = []
    
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            predictions.extend(outputs.cpu().numpy().flatten())
            targets.extend(labels.numpy().flatten())
    
    predictions = np.array(predictions)
    targets = np.array(targets)
    
    rmse = float(np.sqrt(np.mean((predictions - targets) ** 2)))
    mae = float(np.mean(np.abs(predictions - targets)))
    corr = float(np.corrcoef(predictions, targets)[0, 1]) if len(predictions) > 1 else 0.0
    
    return rmse, mae, corr, predictions, targets


def main():
    # 1. Charger les donn√©es
    all_fens, all_evaluations = load_data(DATASET_PATH)
    
    print(f"\nüìä Dataset complet: {len(all_fens):,} positions")
    
    eval_mean = float(np.mean(all_evaluations))
    
    # 2. Initialiser le mod√®le
    if RESET_WEIGHTS and os.path.exists(WEIGHTS_FILE):
        print(f"üóëÔ∏è  Suppression des anciens poids: {WEIGHTS_FILE}")
        os.remove(WEIGHTS_FILE)
    
    if os.path.exists(CHECKPOINT_FILE):
        print(f"üì• Chargement du checkpoint PyTorch: {CHECKPOINT_FILE}")
        model = TorchNNEvaluator(hidden_size=sys.modules[__name__].HIDDEN_SIZE, dropout=DROPOUT, leaky_alpha=LEAKY_ALPHA)
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        model, _, start_step = torch_load_checkpoint(CHECKPOINT_FILE, model, optimizer, device=DEVICE)
        # Also read checkpoint metadata (best_rmse) if present so we don't reset it
        try:
            ckpt_raw = torch.load(CHECKPOINT_FILE, map_location=DEVICE)
            # Determine whether the checkpoint actually contains model weights.
            # We base the validity of `best_rmse` on that: if the checkpoint has no
            # 'model' entry (or it's empty), the stored best_rmse is considered
            # stale and will be removed.
            has_model_weights = 'model' in ckpt_raw and bool(ckpt_raw.get('model'))
            if 'best_rmse' in ckpt_raw and not has_model_weights:
                print(f"‚ö†Ô∏è  Checkpoint contient 'best_rmse' mais ne contient pas de poids -> suppression du meilleur RMSE pour √©viter m√©trique obsol√®te.")
                ckpt_copy = dict(ckpt_raw)
                ckpt_copy.pop('best_rmse', None)
                try:
                    torch.save(ckpt_copy, CHECKPOINT_FILE)
                    print(f"‚úÖ best_rmse supprim√© du checkpoint {CHECKPOINT_FILE}")
                except Exception as ex:
                    print(f"‚ö†Ô∏è Impossible de r√©√©crire le checkpoint pour supprimer best_rmse: {ex}")
                best_rmse = float('inf')
            else:
                best_rmse = float(ckpt_raw.get('best_rmse', float('inf')))
        except Exception:
            best_rmse = float('inf')

        print(f"‚úÖ Checkpoint charg√© (step {start_step}), best_rmse={best_rmse}")
    elif os.path.exists(WEIGHTS_FILE):
        print(f"üì• Chargement des poids NumPy: {WEIGHTS_FILE}")
        model, adam_moments = load_from_npz(WEIGHTS_FILE, device=str(DEVICE))
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        # If the saved .npz contains metadata (learning_rate / best_rmse), apply it
        if adam_moments is not None:
            # learning_rate saved as float in our save convention
            if 'learning_rate' in adam_moments:
                try:
                    lr_saved = float(adam_moments['learning_rate'])
                    for pg in optimizer.param_groups:
                        pg['lr'] = lr_saved
                    print(f"‚ÑπÔ∏è LR restaur√© depuis {WEIGHTS_FILE}: {lr_saved:.6f}")
                except Exception:
                    pass
            if 'best_rmse' in adam_moments:
                try:
                    best_rmse = float(adam_moments['best_rmse'])
                    print(f"‚ÑπÔ∏è best_rmse restaur√© depuis {WEIGHTS_FILE}: {best_rmse:.4f}")
                except Exception:
                    best_rmse = float('inf')
        # TODO: Restaurer les moments Adam si pr√©sents (d√©pend de correspondance de forme)
        # If we have a checkpoint metadata file with a best_rmse and the npz exists,
        # allow loading that best_rmse so training can resume with the historical metric.
        try:
            if os.path.exists(CHECKPOINT_FILE):
                ckpt_raw = torch.load(CHECKPOINT_FILE, map_location=DEVICE)
                # Only honor best_rmse from the checkpoint if the checkpoint
                # actually contains model weights.
                has_model_weights = 'model' in ckpt_raw and bool(ckpt_raw.get('model'))
                if has_model_weights:
                    best_rmse = float(ckpt_raw.get('best_rmse', float('inf')))
                else:
                    best_rmse = float('inf')
            else:
                best_rmse = float('inf')
        except Exception:
            best_rmse = float('inf')

        print(f"‚úÖ Poids charg√©s depuis NumPy (best_rmse initial={best_rmse})")
    else:
        print("üÜï Cr√©ation d'un nouveau r√©seau...")
        model = TorchNNEvaluator(hidden_size=HIDDEN_SIZE, dropout=DROPOUT, leaky_alpha=LEAKY_ALPHA)
        # Initialisation He (PyTorch le fait d√©j√† par d√©faut pour Linear + ReLU)
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        
        # Warm-start du biais de sortie
        with torch.no_grad():
            model.l3.bias[0] = eval_mean
    
    model.to(DEVICE)
    # --- Interactive LR selection at start ---
    # If a previous run saved an optimizer LR in the checkpoint or a metadata key
    # in the .npz, show it and allow the user to enter a LR to start with.
    recorded_lr = None
    try:
        if os.path.exists(CHECKPOINT_FILE):
            ck = torch.load(CHECKPOINT_FILE, map_location='cpu')
            optim_raw = ck.get('optim')
            if optim_raw and isinstance(optim_raw, dict):
                pgs = optim_raw.get('param_groups')
                if pgs and len(pgs) > 0:
                    recorded_lr = float(pgs[0].get('lr', recorded_lr))
    except Exception:
        recorded_lr = recorded_lr

    # fallback to .npz metadata if present
    try:
        if recorded_lr is None and os.path.exists(WEIGHTS_FILE):
            data = np.load(WEIGHTS_FILE)
            if 'learning_rate' in data:
                recorded_lr = float(data['learning_rate'])
    except Exception:
        recorded_lr = recorded_lr

    default_lr = recorded_lr if recorded_lr is not None else LEARNING_RATE
    if recorded_lr is not None:
        print(f"‚ÑπÔ∏è Learning rate enregistr√© d√©tect√©: {recorded_lr:.6f}")
    else:
        print(f"‚ÑπÔ∏è Aucun learning rate enregistr√© trouv√©; valeur par d√©faut: {default_lr:.6f}")

    try:
        # interactive prompt: user can press Enter to accept default
        ui = input(f"Saisir le learning rate de d√©part [{default_lr:.6f}]: ").strip()
        if ui != "":
            chosen_lr = float(ui)
        else:
            chosen_lr = float(default_lr)
    except Exception:
        print("‚ö†Ô∏è Entr√©e invalide ‚Äî utilisation de la valeur par d√©faut")
        chosen_lr = float(default_lr)

    # Apply chosen LR to optimizer param groups
    try:
        for pg in optimizer.param_groups:
            pg['lr'] = chosen_lr
        print(f"‚û°Ô∏è Learning rate initial utilis√© pour l'entra√Ænement: {chosen_lr:.6f}")
    except Exception:
        print("‚ö†Ô∏è Impossible d'appliquer le learning rate √† l'optimizer; v√©rifiez l'√©tat de l'optimizer")
    
    # 3. Configuration de l'entra√Ænement
    criterion = nn.MSELoss()
    
    # LR Scheduler
    if USE_LR_SCHEDULER:
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=LR_FACTOR, 
            patience=LR_PATIENCE
        )
    
    print(f"\n{'='*70}")
    print(f"Configuration:")
    print(f"  Dataset complet: {len(all_fens):,} positions")
    print(f"  √âchantillon/epoch: {MAX_SAMPLES if USE_SAMPLING else len(all_fens):,} positions")
    print(f"  Architecture: 768 ‚Üí {HIDDEN_SIZE} ‚Üí {HIDDEN_SIZE} ‚Üí 1")
    print(f"  Dropout: {DROPOUT}")
    print(f"  LeakyReLU alpha: {LEAKY_ALPHA}")
    print(f"  Learning rate: {LEARNING_RATE} (AdamW, weight decay: {WEIGHT_DECAY})")
    print(f"  LR Warmup: {USE_LR_WARMUP} ({WARMUP_START_LR if USE_LR_WARMUP else 'N/A'} ‚Üí {LEARNING_RATE})")
    print(f"  LR Scheduler: {USE_LR_SCHEDULER} (patience: {LR_PATIENCE if USE_LR_SCHEDULER else 'N/A'})")
    print(f"  Batch size: {BATCH_SIZE}")
    print(f"  Epochs: {EPOCHS}")
    print(f"  Device: {DEVICE}")
    print(f"{'='*70}\n")
    # 4. Boucle d'entra√Ænement
    # If a checkpoint or weights file existed at startup we treat this run as
    # a resume. In that case we disable the LR warmup and reuse the optimizer
    # LR (if restored from checkpoint) or the metadata saved in the .npz.
    resumed = False
    effective_use_lr_warmup = USE_LR_WARMUP
    if os.path.exists(CHECKPOINT_FILE) or os.path.exists(WEIGHTS_FILE):
        resumed = True
        effective_use_lr_warmup = False
        print("‚ÑπÔ∏è Reprise d√©tect√©e: d√©sactivation du LR warmup pour utiliser le LR sauvegard√©/pr√©c√©dent.")

    # best_rmse may have been set during checkpoint/weights loading above; default to +inf
    best_rmse = locals().get('best_rmse', float('inf'))

    for epoch in range(EPOCHS):
        # √âchantillonnage √† chaque epoch
        if USE_SAMPLING and len(all_fens) > MAX_SAMPLES:
            print(f"\n[Epoch {epoch+1}] üé≤ √âchantillonnage: {MAX_SAMPLES:,} positions sur {len(all_fens):,}")
            idx = np.random.choice(len(all_fens), size=MAX_SAMPLES, replace=False)
            fens = all_fens[idx]
            evaluations = all_evaluations[idx]
        else:
            fens = all_fens
            evaluations = all_evaluations
        
        # LR Warmup
        if effective_use_lr_warmup and epoch < WARMUP_EPOCHS:
            warmup_progress = (epoch + 1) / WARMUP_EPOCHS
            lr = WARMUP_START_LR + (LEARNING_RATE - WARMUP_START_LR) * warmup_progress
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr
            print(f"üî• Warmup epoch {epoch+1}/{WARMUP_EPOCHS}: LR = {lr:.6f}")

        # Afficher le LR courant utilis√© pour cette √©poque (apr√®s warmup si applicable)
        try:
            lrs = [pg.get('lr', None) for pg in optimizer.param_groups]
            if len(lrs) == 1:
                print(f"‚û°Ô∏è Learning rate courant: {lrs[0]:.6f}")
            else:
                print("‚û°Ô∏è Learning rates courants: " + ", ".join(f"{x:.6f}" for x in lrs))
        except Exception:
            # D√©fensif: si optimizer absent ou structure inattendue, afficher l'hyperparam√®tre initial
            print(f"‚û°Ô∏è Learning rate (approx): {LEARNING_RATE}")
        
        # Cr√©er le dataset et dataloader
        train_dataset = ChessDataset(fens, evaluations)
        train_loader = DataLoader(
            train_dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=True,
            num_workers=0,  # Augmenter si CPU multi-core (ex: 4)
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        # Training
        model.train()
        total_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}")
        for batch_idx, (inputs, targets) in enumerate(progress_bar):
            
            # Check for NaNs/Infs before moving to device
            if torch.isnan(inputs).any() or torch.isinf(inputs).any():
                print(f"‚ö†Ô∏è WARNING: NaN or Inf found in inputs for batch {batch_idx}, epoch {epoch+1}. Skipping batch.")
                # Optional: Add more info about the problematic data
                # print("Problematic inputs:", inputs[torch.isnan(inputs) | torch.isinf(inputs)])
                continue # Skip this batch

            if torch.isnan(targets).any() or torch.isinf(targets).any():
                print(f"‚ö†Ô∏è WARNING: NaN or Inf found in targets for batch {batch_idx}, epoch {epoch+1}. Skipping batch.")
                # Optional: Add more info about the problematic data
                # print("Problematic targets:", targets[torch.isnan(targets) | torch.isinf(targets)])
                continue # Skip this batch

            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            
            # Forward
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            # Backward
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            
            # Update
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # Debug stats (premier batch)
            if DEBUG_STATS and epoch == 0 and batch_idx == 0:
                with torch.no_grad():
                    preds = outputs.cpu().numpy().flatten()
                    targs = targets.cpu().numpy().flatten()
                    batch_rmse = np.sqrt(np.mean((preds - targs) ** 2))
                    corr = np.corrcoef(preds, targs)[0, 1] if len(preds) > 1 else 0.0
                    print(f"\n[DEBUG batch 0] targets mean={targs.mean():.4f} std={targs.std():.4f}; "
                          f"preds mean={preds.mean():.4f} std={preds.std():.4f}; "
                          f"RMSE={batch_rmse:.4f}; corr={corr:.4f}")
            
            # Update progress bar
            avg_loss = total_loss / num_batches
            progress_bar.set_postfix({"loss": f"{np.sqrt(avg_loss):.4f}"})
        
        # √âvaluation fin d'√©poque
        print(f"\nüîç √âvaluation epoch {epoch+1}...")
        
        # √âchantillon d'√©valuation
        if EVAL_MAX_SAMPLES and len(all_fens) > EVAL_MAX_SAMPLES:
            eval_idx = np.random.choice(len(all_fens), size=EVAL_MAX_SAMPLES, replace=False)
            eval_fens = all_fens[eval_idx]
            eval_targets = all_evaluations[eval_idx]
        else:
            eval_fens = all_fens
            eval_targets = all_evaluations
        
        eval_dataset = ChessDataset(eval_fens, eval_targets)
        eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE*2, shuffle=False)
        
        rmse, mae, corr, preds, targets = evaluate_model(model, eval_loader, DEVICE)
        
        baseline_rmse = targets.std()
        improvement = 100 * (1 - rmse / baseline_rmse) if baseline_rmse > 0 else 0
        
        # Affichage
        print(f"\n{'='*70}")
        print(f"EPOCH {epoch+1}/{EPOCHS} - √âvaluation sur {len(eval_fens):,} positions")
        print(f"{'='*70}")
        print(f"  RMSE:        {rmse:.4f}  (baseline: {baseline_rmse:.4f})")
        print(f"  MAE:         {mae:.4f}")
        print(f"  Am√©lioration: {improvement:+.1f}% vs baseline")
        print(f"  Corr√©lation: {corr:.4f}")
        print(f"  Std preds:   {preds.std():.4f}  (cible: {targets.std():.4f})")
        print(f"  Mean preds:  {preds.mean():.4f}  (cible: {targets.mean():.4f})")
        
        if improvement > 50:
            print(f"  ‚úì‚úì Performance excellente!")
        elif improvement > 30:
            print(f"  ‚úì  Bon apprentissage!")
        elif improvement > 10:
            print(f"  ‚Üí  Apprentissage en cours")
        else:
            print(f"  ‚ö†  Faible am√©lioration - v√©rifier hyperparam√®tres")
        print(f"{'='*70}\n")
        
        # LR Scheduler
        if USE_LR_SCHEDULER and (not effective_use_lr_warmup or epoch >= WARMUP_EPOCHS):
            scheduler.step(rmse)
        
        # Sauvegarder le meilleur mod√®le
        if rmse < best_rmse:
            best_rmse = rmse
            print(f"üíæ Nouveau meilleur RMSE: {best_rmse:.4f} - Sauvegarde...")
            torch_save_checkpoint(CHECKPOINT_FILE, model, optimizer, epoch, best_rmse=best_rmse)
            # Save weights + metadata (learning_rate and best_rmse) so next runs can resume
            try:
                current_lr = float(optimizer.param_groups[0].get('lr', LEARNING_RATE))
            except Exception:
                current_lr = LEARNING_RATE
            meta = {'learning_rate': current_lr, 'best_rmse': float(best_rmse)}
            save_weights_npz(model, WEIGHTS_FILE, adam_moments=meta)
    
    print("\nüéâ Entra√Ænement termin√©!")
    print(f"üìä Meilleur RMSE: {best_rmse:.4f}")
    
    # Sauvegarde finale
    print(f"\nüíæ Sauvegarde finale...")
    torch_save_checkpoint(CHECKPOINT_FILE, model, optimizer, EPOCHS, best_rmse=best_rmse)
    try:
        current_lr = float(optimizer.param_groups[0].get('lr', LEARNING_RATE))
    except Exception:
        current_lr = LEARNING_RATE
    meta = {'learning_rate': current_lr, 'best_rmse': float(best_rmse)}
    save_weights_npz(model, WEIGHTS_FILE, adam_moments=meta)
    print(f"‚úÖ Mod√®le sauvegard√© dans {CHECKPOINT_FILE} et {WEIGHTS_FILE}")


if __name__ == "__main__":
    main()
