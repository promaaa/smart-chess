# Guide des ressources mat√©rielles pour l'entra√Ænement

## üìä Ton contexte actuel

**Dataset** : 13 millions de positions
**Temps par epoch** : 4 heures (CPU only, ~55k positions trait√©es)
**Architecture** : 768 ‚Üí 256 ‚Üí 256 ‚Üí 1 (~263k param√®tres)

## üîç Analyse du probl√®me

### Probl√®me principal : **Boucle non-vectoris√©e**
```python
# Code actuel (LENT) ‚ùå
for fen in batch:
    x = encode(fen)  # Une position √† la fois
    forward_pass(x)   # NumPy sur 1 position
    backward_pass()
```

**Impact** : 13M appels Python ‚Üí overhead √©norme

### Solution 1 : **√âchantillonnage (appliqu√©)** ‚úì
```python
MAX_SAMPLES = 200,000  # 1.5% du dataset
Temps par epoch : ~10-15 minutes (au lieu de 4h)
```

**Pourquoi √ßa suffit** :
- 200k positions bien √©chantillonn√©es ‚âà diversit√© du dataset complet
- Plus d'epochs sur moins de data > moins d'epochs sur plus de data
- It√©ration rapide pour tuner hyperparam√®tres

## ‚ö° Ressources mat√©rielles recommand√©es

### **Option 1 : CPU moderne (ton cas actuel)**

**Minimum viable** :
- CPU : Intel i5/i7 ou AMD Ryzen 5/7 (4+ cores)
- RAM : 8 GB minimum, 16 GB recommand√©
- Storage : SSD pour chargement rapide du CSV

**Performance avec 200k positions** :
```
Temps par epoch : 10-15 minutes
20 epochs : ~3-5 heures total
Performance attendue : RMSE ~0.30-0.35 (tr√®s bon)
```

**Performance avec 13M positions** :
```
Impossible en pratique sans vectorisation GPU
M√™me avec CPU puissant : plusieurs jours par epoch
```

### **Option 2 : GPU consumer (recommand√© si s√©rieux)**

**Config recommand√©e** :
- GPU : NVIDIA RTX 3060 (12GB), RTX 4060 Ti, ou mieux
- RAM : 16 GB
- Framework : PyTorch ou TensorFlow

**Performance avec 200k positions** :
```
Temps par epoch : 1-2 minutes (10x plus rapide)
20 epochs : ~30-40 minutes total
```

**Performance avec 13M positions** :
```
Temps par epoch : 30-60 minutes
20 epochs : 10-20 heures total
Faisable avec batching GPU optimis√©
```

**Co√ªt** : ~400-600‚Ç¨ pour RTX 3060/4060 Ti

### **Option 3 : Cloud GPU (flexible)**

**Google Colab** :
- Gratuit : T4 GPU (15 GB), ~12h/jour
- Pro ($10/mois) : A100 GPU, temps illimit√©
- Performance : 50-100x plus rapide que CPU

**AWS/Azure/GCP** :
- ~$0.50-2/heure selon GPU
- Bon pour exp√©rimenter sans investir

**Exemple AWS** :
```
Instance : p3.2xlarge (Tesla V100)
Prix : ~$3/heure
Temps 13M positions : ~5-10 heures total (tous epochs)
Co√ªt total : ~$15-30
```

### **Option 4 : Laptop gaming moderne**

Si tu as d√©j√† :
- RTX 3050/3060 laptop
- 16 GB RAM
- ‚Üí Parfait pour 200k-500k positions

## üéØ Mes recommandations par budget

### **Budget 0‚Ç¨ : Reste CPU + √©chantillonnage** ‚úì
```python
MAX_SAMPLES = 200,000
20 epochs √ó 12 minutes = 4 heures
```
**R√©sultat** : Mod√®le tr√®s performant (~1500 Elo)

### **Budget 10‚Ç¨/mois : Google Colab Pro**
```python
MAX_SAMPLES = 1,000,000
20 epochs √ó 5 minutes = 1h40
```
**R√©sultat** : Mod√®le excellent (~1700-1800 Elo)

### **Budget 500‚Ç¨ : Acheter RTX 3060**
```python
MAX_SAMPLES = 13,000,000 (tout!)
20 epochs √ó 45 minutes = 15 heures
```
**R√©sultat** : Mod√®le top (~1800-2000 Elo)
**Bonus** : Utilisable pour autres projets ML/gaming

## üìà Comparaison performance vs dataset

| Positions | Epochs | Temps CPU | Temps GPU (RTX 3060) | RMSE attendu |
|-----------|--------|-----------|---------------------|--------------|
| 50k       | 20     | 2h        | 15 min              | ~0.35-0.40   |
| 200k ‚úì    | 20     | 4-5h      | 30 min              | ~0.30-0.35   |
| 500k      | 20     | 10-12h    | 1h                  | ~0.28-0.32   |
| 1M        | 20     | 20-24h    | 2h                  | ~0.27-0.31   |
| 13M       | 20     | Impossible| 15-20h              | ~0.25-0.30   |

**Rendements d√©croissants** : Passer de 200k √† 13M ne divise la loss que par ~1.15x

## üí° Strat√©gie optimale pour toi

### **Phase 1 : Validation (maintenant)** ‚úì
```python
MAX_SAMPLES = 200,000
EPOCHS = 20
```
**But** : Prouver que l'architecture + warmup marche
**Temps** : ~4-5h sur ton CPU
**Co√ªt** : 0‚Ç¨

### **Phase 2 : Am√©lioration (optionnel)**
```python
MAX_SAMPLES = 500,000
EPOCHS = 30
```
**Option A** : CPU overnight (10-12h)
**Option B** : Google Colab gratuit (2h)
**But** : Atteindre RMSE ~0.28-0.30

### **Phase 3 : Maximum (si vraiment motiv√©)**
```python
MAX_SAMPLES = 13,000,000
EPOCHS = 50
```
**Requis** : GPU (Colab Pro ou hardware)
**Temps** : 20-30h total
**But** : Niveau comp√©titif (~2000 Elo)

## üöÄ Actions imm√©diates

1. **Lance avec 200k √©chantillons** (d√©j√† configur√©)
2. **Attends les r√©sultats** (4-5h)
3. **Si RMSE < 0.35** ‚Üí Mission accomplie ! ‚úì
4. **Si tu veux pousser** ‚Üí Passe √† 500k ou essaye Colab

## üìù Note importante

**Avoir plus de donn√©es n'am√©liore pas toujours la performance** :
- 200k positions bien √©chantillonn√©es > 13M positions mal utilis√©es
- Plus d'epochs sur moins de data souvent meilleur
- It√©ration rapide > attendre 20h par run

**Stockfish NNUE** utilise 500M positions mais avec :
- Architecture sp√©cialis√©e NNUE
- Self-play reinforcement learning
- Cluster de serveurs
- Mois d'entra√Ænement

Pour ton projet, **200k-500k positions suffisent largement** ! üéØ
