# Guide d'entra√Ænement du r√©seau de neurones d'√©valuation

## üéØ M√©triques √† surveiller

### 1. RMSE (Root Mean Square Error)
- **D√©finition**: Erreur quadratique moyenne entre pr√©dictions et √©valuations r√©elles
- **√âchelle**: Normalis√© (divis√© par 1000), donc 1.0 = 1000 centipawns d'erreur
- **Baseline**: ~0.9-1.0 (toujours pr√©dire la moyenne)
- **Objectifs**:
  - Epoch 1: ~0.5-0.7 ‚úì
  - Epoch 5: ~0.4-0.5 ‚úì
  - Epoch 10: ~0.3-0.4 ‚úì (excellent)

### 2. Corr√©lation
- **D√©finition**: Corr√©lation de Pearson entre pr√©dictions et cibles
- **√âchelle**: -1 √† +1 (1 = pr√©diction parfaite, 0 = al√©atoire)
- **Objectifs**:
  - Epoch 1-2: >0.1-0.2 (d√©but d'apprentissage)
  - Epoch 5: >0.3-0.4 (bon)
  - Epoch 10: >0.5-0.7 (excellent)

### 3. Am√©lioration vs Baseline
- **D√©finition**: % de r√©duction d'erreur par rapport √† toujours pr√©dire la moyenne
- **Objectifs**:
  - Epoch 1: >20-30% ‚úì
  - Epoch 5: >40-50% ‚úì
  - Epoch 10: >55-65% ‚úì (excellent)

### 4. Std(preds) vs Std(targets)
- **D√©finition**: √âcart-type des pr√©dictions compar√© aux cibles
- **Cible**: ~0.8-1.0 (m√™me ordre de grandeur que les √©valuations)
- **Probl√®me**: Si std(preds) << std(targets), le mod√®le manque de variance (pr√©dictions trop "plates")

## üöÄ Configuration actuelle

```python
Architecture: 768 ‚Üí 128 (ReLU) ‚Üí 128 (ReLU) ‚Üí 1
Optimiseur: Adam (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999, Œµ=1e-8)
Learning rate: 0.0005
Batch size: 64
Gradient clipping: 5.0 (norme globale)
Normalisation: √âvals / 1000.0
```

## üìä Interpr√©tation des r√©sultats

### ‚úÖ Signes de bon apprentissage
- RMSE descend progressivement entre epochs
- Corr√©lation monte progressivement
- Am√©lioration vs baseline > 30% apr√®s quelques epochs
- Std(preds) se rapproche de std(targets) (~0.8-1.0)

### ‚ö†Ô∏è Signes de probl√®mes
- RMSE stagne ou augmente
- Corr√©lation < 0.1 apr√®s 5+ epochs
- Am√©lioration vs baseline < 10%
- Std(preds) reste tr√®s faible (~0.1-0.2)

### üîß Actions correctives

**Si loss stagne haut (>0.7):**
- Augmenter le learning rate √† 0.001
- Augmenter la taille des couches cach√©es √† 256
- V√©rifier que les donn√©es sont bien charg√©es

**Si corr√©lation reste faible (<0.2):**
- V√©rifier la normalisation des donn√©es
- Augmenter le nombre d'epochs
- Essayer d'augmenter la capacit√© du r√©seau

**Si std(preds) trop faible:**
- D√©j√† g√©r√© par warm-start du biais de sortie
- Adam aide √† adapter les learning rates par param√®tre

## üìà Commandes utiles

### Lancer l'entra√Ænement
```bash
cd "c:\Users\gauti\OneDrive\Documents\UE commande\smart-chess\ai"
python train.py
```

### V√©rifier les statistiques du dataset
```bash
python check_dataset_stats.py
```

### Tester le mod√®le entra√Æn√©
```python
from nn_evaluator import load_evaluator_from_file
from Chess import Chess

evaluator = load_evaluator_from_file("chess_nn_weights.npz")
chess = Chess()
chess.load_fen("rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1")
score = evaluator.evaluate_position(chess)
print(f"√âvaluation: {score:.0f} centipawns")
```

## üéì Conseils

1. **Patience**: L'apprentissage sur un gros dataset prend du temps
2. **Surveillance**: Regarde les m√©triques de fin d'epoch pour suivre la progression
3. **Sauvegarde**: Les poids sont sauvegard√©s apr√®s chaque epoch
4. **Continuit√©**: Si interrompu, l'entra√Ænement reprend depuis les derniers poids
5. **Validation**: Teste ton mod√®le sur des positions connues pour v√©rifier qu'il est sens√©

## üìù Notes sur l'√©chelle

- **Stockage**: √âvaluations en centipawns (ex: +500 = avantage d'un pion)
- **Entra√Ænement**: Normalis√© √∑1000 (ex: +500 ‚Üí 0.5)
- **Inf√©rence**: Re-multipli√© √ó1000 (ex: 0.5 ‚Üí +500 centipawns)

Cette normalisation stabilise l'entra√Ænement en gardant les gradients dans une plage raisonnable.
